{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4d151c",
   "metadata": {
    "papermill": {
     "duration": 0.004876,
     "end_time": "2023-06-03T09:33:31.705688",
     "exception": false,
     "start_time": "2023-06-03T09:33:31.700812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MobileFaceNet \n",
    "\n",
    "Facial recognition is one of the most commonly used computer-vision task in our daily life.  \n",
    "A facial recognition model takes a close-up photo of a face and calculates an embedding value(a 128-dimensonal vector) for the face, the embedding is then compared to a database of known faces using a distance function(e.g. Euclidean distance), in order to find out the closest known face and hence the person can be identified.  \n",
    "**[MobileFaceNet](https://arxiv.org/abs/1804.07573)** is a face verification that is designed to run on mobile devices with abount 1 million parameters. \n",
    "There are many implementations available on Github but most of them are old and based on Tensorflow version 1.  \n",
    "This is a re-implementation of [MobileFaceNet_TF repo](https://github.com/sirius-ai/MobileFaceNet_TF) using Tensorflow version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cda9e8a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-03T09:33:31.715995Z",
     "iopub.status.busy": "2023-06-03T09:33:31.715457Z",
     "iopub.status.idle": "2023-06-03T09:33:40.091016Z",
     "shell.execute_reply": "2023-06-03T09:33:40.089575Z"
    },
    "papermill": {
     "duration": 8.384416,
     "end_time": "2023-06-03T09:33:40.094269",
     "exception": false,
     "start_time": "2023-06-03T09:33:31.709853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global batch size: 512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Model, mixed_precision\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, DepthwiseConv2D, ZeroPadding2D, BatchNormalization, ReLU, Add, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# mixed precision training is used\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# 128 for 3060M, 512 for P100 \n",
    "GLOBAL_BATCH_SIZE = 512\n",
    "print('Global batch size: {}'.format(GLOBAL_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1ca4b",
   "metadata": {
    "papermill": {
     "duration": 0.003661,
     "end_time": "2023-06-03T09:33:40.101857",
     "exception": false,
     "start_time": "2023-06-03T09:33:40.098196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Training Dataset\n",
    "The training dataset MS1M-refine-v2(a.k.a MS1M-ArcFace) come from [here](https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_)              \n",
    "The dataset has been converted into TFRecord dataset format.  \n",
    "A parse function is used to convert the examples in to TFRecord dataset into the format required for training.  \n",
    "Data augmentation can also be implemented in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cb9cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T09:33:40.111371Z",
     "iopub.status.busy": "2023-06-03T09:33:40.110621Z",
     "iopub.status.idle": "2023-06-03T09:33:42.679302Z",
     "shell.execute_reply": "2023-06-03T09:33:42.678106Z"
    },
    "papermill": {
     "duration": 2.576765,
     "end_time": "2023-06-03T09:33:42.682293",
     "exception": false,
     "start_time": "2023-06-03T09:33:40.105528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_function(example_proto, n_classes=85742):\n",
    "    features = {'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "                'label': tf.io.FixedLenFeature([], tf.int64)}\n",
    "    features = tf.io.parse_single_example(example_proto, features)\n",
    "    img = tf.image.decode_jpeg(features['image_raw'])\n",
    "    img = tf.reshape(img, shape=(112, 112, 3))\n",
    "\n",
    "    # You can do more image distortion here for training data\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    #img = tf.subtract(img, 127.5)\n",
    "    img = tf.subtract(img, 128)\n",
    "    img = tf.multiply(img,  0.0078125)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    label = tf.cast(features['label'], tf.int64)\n",
    "    label = tf.one_hot(label, n_classes)\n",
    "    return (img, label), label\n",
    "\n",
    "\n",
    "file_dataset = tf.data.TFRecordDataset.list_files(\"/kaggle/input/faces-ms1m-refine-v2-112x112-tfrecord/faces_ms1m_refine_v2_112x112-*.tfrecord\")\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(file_dataset,num_parallel_reads=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(GLOBAL_BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb54b8",
   "metadata": {
    "papermill": {
     "duration": 0.003641,
     "end_time": "2023-06-03T09:33:42.689953",
     "exception": false,
     "start_time": "2023-06-03T09:33:42.686312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the Model and ArcFace loss function\n",
    "Here defines the model and the loss function.  \n",
    "Most of the codes come from https://github.com/sirius-ai/MobileFaceNet_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fbf625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T09:33:42.699399Z",
     "iopub.status.busy": "2023-06-03T09:33:42.699033Z",
     "iopub.status.idle": "2023-06-03T09:33:42.747123Z",
     "shell.execute_reply": "2023-06-03T09:33:42.746088Z"
    },
    "papermill": {
     "duration": 0.056396,
     "end_time": "2023-06-03T09:33:42.749958",
     "exception": false,
     "start_time": "2023-06-03T09:33:42.693562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Model\n",
    "class ArcFace(Layer):\n",
    "    def __init__(self, n_classes=10, s=64.0, m=0.50, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "\n",
    "        embedding, labels = inputs\n",
    "\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.divide(embedding, embedding_norm, name='norm_embedding')\n",
    "        weights = self.W\n",
    "        weights_norm = tf.norm(weights, axis=0, keepdims=True)\n",
    "        weights = tf.divide(weights, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        mask = labels\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "\n",
    "        logit = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "        out = tf.nn.softmax(logit)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'s': self.s, 'm': self.m, 'n_classes': self.n_classes}\n",
    "\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 2 if tf.keras.backend.image_data_format() == 'channels_first' else 1\n",
    "    input_size = tf.keras.backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "\n",
    "def inverted_res_block(inputs, expansion, stride, filters, block_id):\n",
    "    channel_axis = -1\n",
    "    in_channels = tf.keras.backend.int_shape(inputs)[channel_axis]\n",
    "    pointwise_filters = filters\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "\n",
    "    # Expand\n",
    "    x = Conv2D(\n",
    "        expansion * in_channels,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'expand')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'expand_BN')(x)\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = ZeroPadding2D(\n",
    "            padding=correct_pad(x, 3),\n",
    "            name=prefix + 'pad')(x)\n",
    "    x = DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        strides=stride,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same' if stride == 1 else 'valid',\n",
    "        name=prefix + 'depthwise')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'depthwise_BN')(x)\n",
    "\n",
    "    x = ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    # Project\n",
    "    x = Conv2D(\n",
    "        pointwise_filters,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + 'project')(x)\n",
    "    x = BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + 'project_BN')(x)\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return Add(name=prefix + 'add')([inputs, x])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def mobilefacenet_arcface(n_classes=85742):\n",
    "    weight_decay=0.00005\n",
    "    \n",
    "    # input\n",
    "    input = Input(shape=(112, 112, 3), name='input')\n",
    "    y = Input(shape=(n_classes), name='label')\n",
    "\n",
    "    # Conv2D\n",
    "    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec1_conv2d')(input)\n",
    "    x = BatchNormalization(name='sec1_bn')(x)\n",
    "    x = ReLU(name='sec1_relu')(x)\n",
    "\n",
    "    # DepthwiseConv2D\n",
    "    x = DepthwiseConv2D((3,3), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec2_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec2_bn')(x)\n",
    "    x = ReLU(name='sec2_relu')(x)\n",
    "    x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec2_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec2_bn2')(x)\n",
    "\n",
    "    # InvertedResidualBlock\n",
    "    x = inverted_res_block(x, 2, 2,  64, 0)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 1)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 2)\n",
    "    x = inverted_res_block(x, 4, 2, 128, 3)\n",
    "    x = inverted_res_block(x, 2, 1, 128, 4)\n",
    "\n",
    "    # Conv2D 1x1\n",
    "    x = Conv2D(512, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec8_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec8_bn')(x)\n",
    "    x = ReLU(name='sec8_relu')(x)\n",
    "\n",
    "    # linear GDConv 7x7\n",
    "    x = DepthwiseConv2D((7,7), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec9_depthwiseconv2d')(x)\n",
    "    x = BatchNormalization(name='sec9_bn')(x)\n",
    "    x = Conv2D(512, (1, 1), padding='valid', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), name='sec9_conv2d', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='sec9_bn2')(x)\n",
    "\n",
    "    # linear Conv2D 1x1\n",
    "    x = Conv2D(128, (1, 1), padding='same', strides=(1, 1), kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(weight_decay), use_bias=False, name='sec10_conv2d')(x)\n",
    "    x = BatchNormalization(name='sec10_bn')(x)\n",
    "\n",
    "    # faltten\n",
    "    x = Flatten(dtype='float32')(x)\n",
    "\n",
    "    # embedding\n",
    "    x = tf.keras.layers.Lambda(lambda k: tf.keras.backend.l2_normalize(k, axis=-1), name=\"embedding\")(x)\n",
    "\n",
    "    # loss function\n",
    "    output = ArcFace(n_classes=85742, dtype='float32')([x, y])\n",
    "\n",
    "    return Model([input, y], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998a177",
   "metadata": {
    "papermill": {
     "duration": 0.003479,
     "end_time": "2023-06-03T09:33:42.757415",
     "exception": false,
     "start_time": "2023-06-03T09:33:42.753936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8440f932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T09:33:42.766536Z",
     "iopub.status.busy": "2023-06-03T09:33:42.766208Z",
     "iopub.status.idle": "2023-06-03T20:22:29.389301Z",
     "shell.execute_reply": "2023-06-03T20:22:29.388217Z"
    },
    "papermill": {
     "duration": 38933.437115,
     "end_time": "2023-06-03T20:22:36.198359",
     "exception": false,
     "start_time": "2023-06-03T09:33:42.761244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 1/7\n",
      "11373/11373 [==============================] - 5360s 469ms/step - loss: 29.8272 - accuracy: 0.0069 - lr: 0.0100\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 2/7\n",
      "11373/11373 [==============================] - 5547s 488ms/step - loss: 17.3079 - accuracy: 0.0393 - lr: 0.0100\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "11373/11373 [==============================] - 5484s 482ms/step - loss: 14.5730 - accuracy: 0.0798 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "11373/11373 [==============================] - 5616s 494ms/step - loss: 14.1389 - accuracy: 0.0870 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "11373/11373 [==============================] - 5611s 493ms/step - loss: 13.9059 - accuracy: 0.0906 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 6/7\n",
      "11373/11373 [==============================] - 5658s 498ms/step - loss: 13.6103 - accuracy: 0.0968 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 7/7\n",
      "11373/11373 [==============================] - 5545s 487ms/step - loss: 13.5845 - accuracy: 0.0972 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e42f0ececd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mobilefacenet_arcface()\n",
    "opt = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#model.summary()\n",
    "checkpoint_filepath = './ckpt/epoch_{epoch:02d}'\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return 0.01\n",
    "    elif epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=False)\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq=\"epoch\")\n",
    "\n",
    "model.fit(train_dataset,\n",
    "    epochs=7,\n",
    "    callbacks=[checkpoint_cb, tensorboard_cb, lr_cb],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f94f85",
   "metadata": {
    "papermill": {
     "duration": 6.807305,
     "end_time": "2023-06-03T20:22:49.757689",
     "exception": false,
     "start_time": "2023-06-03T20:22:42.950384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save models in Tensorflow SavedModel format and TFLite format \n",
    "The model used for training includes the ArcFace loss function as the last layer. However, the loss function is not required for inference.  \n",
    "We have to rebuild an output model which does not include the loss function and use the embedding layer as the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccdda61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T20:23:03.745821Z",
     "iopub.status.busy": "2023-06-03T20:23:03.745139Z",
     "iopub.status.idle": "2023-06-03T20:23:28.439216Z",
     "shell.execute_reply": "2023-06-03T20:23:28.438010Z"
    },
    "papermill": {
     "duration": 31.937536,
     "end_time": "2023-06-03T20:23:28.442459",
     "exception": false,
     "start_time": "2023-06-03T20:22:56.504923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the global policy to fp32 and rebuild a fp32 model, this is required for exporting to TFLite model\n",
    "# see issue at https://github.com/tensorflow/tensorflow/issues/46380\n",
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "f32_model = mobilefacenet_arcface()\n",
    "f32_model.set_weights(model.get_weights())\n",
    "\n",
    "# extract the output model from the model used for training\n",
    "outputModel = tf.keras.Model(f32_model.get_layer('input').input, f32_model.get_layer('embedding').output, trainable=False)\n",
    "\n",
    "# save the trained model as TensorFlow SavedModel format\n",
    "outputModel.save('/kaggle/working/output_model')\n",
    "\n",
    "# Convert the output model into a TensorflowLite model and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(outputModel)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('/kaggle/working/output_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39019.261258,
   "end_time": "2023-06-03T20:23:39.948502",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-03T09:33:20.687244",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
